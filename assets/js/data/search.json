[ { "title": "Scraping Together a Triathlon Training Plan", "url": "/posts/triathlon-training/", "categories": "Web Scraping", "tags": "python, web-scraping, beautifulsoup", "date": "2024-03-14 00:00:00 +1300", "snippet": "I signed up to compete in a half-Ironman(tm) triathlon on September 8, 2024. Completing a half-Ironman(tm) involves: 1.2 mile swim 56 mile bike 13.1 mile runIt’s nice to have a solid training plan to prepare for such a demanding event, and many kind folks have compiled and published these online for public reference. Rather than constantly referring to a cluttered website, though, I wanted to put all of the recommended workouts on my calendar. I like my calendar.By the power of python, beautifulsoup, and pandas, I’ve done just that - scraped my selected training plan from the web and re-arranged the information into a format which can be imported into Google Calendar, as I prefer.How do?Retrieve and parse the HTML, referring to my browser’s Inspect utility to analyse relevant page structure.import requestsimport bs4import pandas as pdURL = &#39;https://www.triathlete.com/training/super-simple-ironman-70-3-triathlon-training-plan/&#39;RACE_DAY = &#39;9/8/2024&#39;OUT_FILE = &#39;~/Desktop/sc.csv&#39;response = requests.get(URL)soup = bs4.BeautifulSoup(response.text)tables = soup.find_all(&#39;table&#39;)len(tables)17Prep data frame and append a new row for each day in each week-table (all tables except the first table).df = pd.DataFrame({&#39;dow&#39;: [], &#39;wo1&#39;: [], &#39;wo2&#39;: [], &#39;wo3&#39;: []})for i, t in enumerate(tables): if i == 0: continue # print(f&#39;Week {i}&#39;) trs = t.find_all(&#39;tr&#39;) for tr in trs: tds = tr.find_all(&#39;td&#39;) # print(&#39; | &#39;.join(td.text for td in tds)) df.loc[df.shape[0]] = [td.text for td in tds] + ([&#39;&#39;] if i &amp;lt; 16 else []) df.head() dow wo1 wo2 wo3 0 Monday Rest 1 Tuesday Bike 40 minutes moderate with 4 x 30-second sp... 2 Wednesday Swim 800 yards total. Main set: 8 x 25 yards, ... Run 4 miles moderate + 2 x 10-second hill spri... 3 Thursday Bike 40 minutes moderate. 4 Friday Swim 800 yards total. Main set: 3 x 100 yards ... Run 4 miles moderate. Given the race day as the end date, back-fill to assign dates until training day 1.df[&#39;start date&#39;] = pd.date_range(end=RACE_DAY, periods=df.shape[0])df.head() dow wo1 wo2 wo3 start date 0 Monday Rest 2024-05-20 1 Tuesday Bike 40 minutes moderate with 4 x 30-second sp... 2024-05-21 2 Wednesday Swim 800 yards total. Main set: 8 x 25 yards, ... Run 4 miles moderate + 2 x 10-second hill spri... 2024-05-22 3 Thursday Bike 40 minutes moderate. 2024-05-23 4 Friday Swim 800 yards total. Main set: 3 x 100 yards ... Run 4 miles moderate. 2024-05-24 I want each workout to have its own event, e.g. multiple workouts in one day should result in multiple events that day.So, melt the multiple wo workout columns into just one, in a long format with repeated dates. Then, drop rows with empty descriptions, so days where there are fewer workouts don’t have extra empty events.pivot = df.melt( id_vars=&#39;start date&#39;, value_name=&#39;description&#39;, value_vars=[&#39;wo1&#39;, &#39;wo2&#39;, &#39;wo3&#39;])pivot_reduced = pivot.drop(&#39;variable&#39;, axis=1).drop(pivot[pivot[&#39;description&#39;] == &#39;&#39;].index)pivot_reduced.head() start date description 0 2024-05-20 Rest 1 2024-05-21 Bike 40 minutes moderate with 4 x 30-second sp... 2 2024-05-22 Swim 800 yards total. Main set: 8 x 25 yards, ... 3 2024-05-23 Bike 40 minutes moderate. 4 2024-05-24 Swim 800 yards total. Main set: 3 x 100 yards ... Use the first word of each workout description as its event subject/title, and re-sort by date for visual review.pivot_reduced[&#39;subject&#39;] = [x[0].upper() for x in pivot_reduced[&#39;description&#39;].str.split()]pivot_reduced.sort_values(&#39;start date&#39;) start date description subject 0 2024-05-20 Rest REST 1 2024-05-21 Bike 40 minutes moderate with 4 x 30-second sp... BIKE 2 2024-05-22 Swim 800 yards total. Main set: 8 x 25 yards, ... SWIM 114 2024-05-22 Run 4 miles moderate + 2 x 10-second hill spri... RUN 3 2024-05-23 Bike 40 minutes moderate. BIKE ... ... ... ... 221 2024-09-06 Run 3 miles easy. RUN 110 2024-09-07 Swim 10 minutes easy with 4 x 30 seconds at ra... SWIM 222 2024-09-07 Bike 10 minutes with 4 x 30 seconds fast. BIKE 334 2024-09-07 Run 10 minutes with 4 x 20 seconds at 90 perce... RUN 111 2024-09-08 RACE DAY RACE 161 rows × 3 columnsWrite dataframe to CSV for calendar import.pivot_reduced.sort_values(&#39;start date&#39;).to_csv(OUT_FILE, index=False)Huzzah" }, { "title": "Progressive Income Tax Visualiser", "url": "/posts/taxvis/", "categories": "Data Visualisation", "tags": "R, graphics, visualisation, RShiny", "date": "2023-12-08 00:00:00 +1300", "snippet": "In this post, I exhibit the progressive income tax visualisation tool I’ve published as a Shiny App.Just Show the ThingWithout ado, here it is: Waxing PoeticI still remember the aha-moment when I finally took the time to understand progressive tax schemes. My prior assumption, like many others’, was that a single tax rate was applied to the total income, depending on which bracket contained the total income. This assumption leads to incorrect inferences about disincentivisation of hard work and striving for higher income.If a promotion were to bump someone’s income up into the next bracket, would they end up taking home less income due to the higher tax rate? No! That would be ridiculous, and if true, would lead me to believe my government was incompetent.In fact, progressive income tax is quite a reasonable solution to the trade-offs between different framings of fairness in taxation. It seems to me that its main weakness lies in its bracketed (discontinuous) nature, which makes it more difficult both to understand and compute than a continuous taxation function.Continuous Progressive TaxationProponents of bracketed progressive taxation point to its ease of calculation. Simply apply the relevant percentage to the relevant income bracket, sum everything up, and hey presto! Total liability! So easy it can be done by hand (ideally with a calculator).It seems just as easy and even more simple to do this with a continuous tax rate function. Simply apply the function to the total income and hey presto! Total liability! So easy it can be done by hand (ideally with a calculator).A simple quadratic function would do the job, and be much more easily interpreted.\\[y = ax^2 + bx + c\\]The intercept \\(c\\) is the minimum tax liability (this can be negative!), with one coefficient \\(b\\) for the “flat” tax rate, and one coefficient \\(a\\) for the smooth increase of tax rate as income increases.Recognising that a “simplifying” change to tax codes is not at all likely to come about via random blog posts by average Joes, I won’t expound further. If you’d like me to, have any clarifying questions, or want to have a riveting chat about tax formulations, just send me a message!" }, { "title": "Reflections on ML", "url": "/posts/ml-reflections/", "categories": "General", "tags": "machine-learning, retrospective", "date": "2022-06-30 00:00:00 +1200", "snippet": "Latent thoughts and specific review regarding an introductory machine learning course taken at the University of Auckland.My PriorSince my first introduction to computers and computer science, one of the brightest flashes of insight I’ve experienced is the inherent humanity of it all. Computers, from high-level languages all the way down to basic electrical design, bear the marks of their creators. But machine learning seems different, in that it aims to produce machines which can not just follow specific instructions from people, but impersonate people by making predictions based on some model of past experience. Because of this, and a slight dose of contempt for the domains in which machine learning has been popularised, I have been suspicious.My UpdateI have found myself far less suspicious, and more intrigued by machine learning after a course of study. By studying the fundamentals of machine learning, and peeking inside a few modeling techniques, I feel that I have upgraded my understanding. And after implementing different types of models, like Naive Bayes and Support Vector Machines, I feel significantly more confident in moving forward with the rest of my study program and career. I remember feeling a similar step change in confidence, after building an Android application from scratch as part of an undergraduate course. Based on these matched experiences, I’ve learned that my confidence in any given arena is greatly boosted by completing a significant and properly contextualising project.Course FormatI am surprised at how appealing the format of the ML course was for me. My previous experience in technical university courses involved a few more low-level implementation projects, and significantly less high-level synthesis of ideas. Working with classmates to produce a tutorial presentation, engaging with other presenting groups, and creating my own questions on the Peerwise platform promoted my understanding of the content more than I expected. As a result, I have enjoyed this first semester of postgraduate study more than any those of undergraduate, though I’m sure the maturity that I’ve gained in the previous few years is a stronger factor in that change.ApplicationI still recognise within me some uncertainty about the usefulness of machine learning in those domains which are most important. As interesting as spam classification problems are, they are not all that critical in the grand scheme of things. Issues like global health and poverty, though messy and less tractable, are arenas where data-driven decision-making could potentially do a lot of good. Thanks in part to the ML course, I am starting to believe that machine learning has a place in more consequential domains. I think I better understand what machine learning is: another set of computational tools which, if used carefully, can be used to do humanity’s difficult work. As a burgeoning machine learning practitioner, I am more excited than ever to learn more." }, { "title": "Support Vector Machines", "url": "/posts/svm/", "categories": "Machine Learning", "tags": "python, sklearn, machine-learning, support-vector-machine, synthetic-data", "date": "2022-06-20 00:00:00 +1200", "snippet": "In this assignment for a machine learning course, I generate synthetic data and explore how the underlying patterns affect SVM tuning parameters.DS1 Design a new dataset DS1 with at least 50 points, for which the selection of the complexity parameter C in a linear SVM makes a difference.import warningswarnings.filterwarnings(&#39;ignore&#39;)import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.datasets import make_classification, make_blobs# generate reproducible datasetnp.random.seed(12345)N_SAMPLES = 60# vertical cluster (dark, class 0)x1 = np.random.normal(scale=0.1, size=30)y1 = x1-np.linspace(-1, 1, 30)# horizontal cluster (light, class 1)x2 = np.linspace(0, 1, 30)y2 = np.random.normal(scale=0.1, size=30)# combine classes into one datasetX = np.stack([np.concatenate([x1,x2]), np.concatenate([y1,y2])]).Ty = np.array([0 for x in x1] + [1 for x in x2])# plot dataplt.scatter(x=X[:,0], y=X[:,1], c=y)plt.show()# save to csv fileDS_array = np.column_stack([X.round(6), y])DS = pd.DataFrame(DS_array).astype({2:&#39;int&#39;})DS.to_csv(&#39;D1.csv&#39;, header=None, index=None) Load the data set DS1, train an SVM with a linear kernel on the full data set, and plot the data set with the decision boundary.from sklearn.svm import SVC# fit, plot, and return SVM with specified kw parametersdef fit_and_plot(X, y, **kwargs): # create an x,y mesh to predict and plot x_min, y_min = X.iloc[:,0:2].min() x_max, y_max = X.iloc[:,0:2].max() grain = (x_max - x_min) / 100 margin = grain * 10 xx, yy = np.meshgrid(np.arange(x_min-margin, x_max+margin, grain), np.arange(y_min-margin, y_max+margin, grain)) # fit linear SVM classifier and predict z svc = SVC(**kwargs).fit(X, y) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) zz = Z.reshape(xx.shape) # scatter plot and decision boundary plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8) plt.scatter(x=X.iloc[:,0], y=X.iloc[:,1], c=y) plt.show() return svc# load custom dataset from save fileDS1 = pd.read_csv(&#39;D1.csv&#39;, header=None)X1 = DS1.iloc[:, 0:2]y1 = DS1.iloc[:, 2]N_SAMPLES1 = DS1.shape[0]svc1 = fit_and_plot(X1, y1, kernel=&#39;linear&#39;) Carry out a leave-1-out cross-validation with an SVM on the dataset. Report the performance on training and test set.from sklearn.model_selection import LeaveOneOut, cross_validate# pretty-print scores from a cross-validation instancedef print_scores(cv): # print both training and test scores for score in [&#39;train_score&#39;, &#39;test_score&#39;]: u, c = np.unique(cv[score], return_counts=True) u = u.round(3) di = dict(zip(u, c)).items() n = sum([v for k,v in di]) # show unique scores and n times those scores were achieved print(&#39;CV &#39;, score, &#39;s: &#39;, &#39;, &#39;.join([&#39;*&#39;.join([str(k),str(v)]) for k,v in di]), sep=&#39;&#39;) # show weighted mean of scores print(&#39;Mean CV&#39;, score, round(sum([k*v for k,v in di])/n, 5), &#39;\\n&#39;) # print scores of linear SVM with default Cloo = LeaveOneOut()cv1 = cross_validate(svc1, X1, y1, cv=loo.split(X1, y1), return_train_score=True)print_scores(cv1)CV train_scores: 0.814*3, 0.831*47, 0.847*10Mean CV train_score 0.83282 CV test_scores: 0.0*11, 1.0*49Mean CV test_score 0.81667 Improve the SVM by changing C. Plot the data set and resulting decision boundary, give the performance.# fit, plot and print scores of linear SVM with increased Csvc1 = fit_and_plot(X1, y1, kernel=&#39;linear&#39;, C=1000)cv1 = cross_validate(svc1, X1, y1, cv=loo.split(X1, y1), return_train_score=True)print_scores(cv1)CV train_scores: 0.864*9, 0.881*47, 0.898*4Mean CV train_score 0.87958 CV test_scores: 0.0*8, 1.0*52Mean CV test_score 0.86667 Discuss what C does and how it improved the SVM in this case.The complexity parameter C tunes the trade-off between hyperplane margin width and training error. A higher value for C penalises more heavily on misclassified points which are far from their correct margin boundary, thus encouraging a tighter fit to the training data. Likewise, a lower C penalises the same points lightly, giving the model less complexity and a decision boundary less biased to the training data.In this case, increasing C further penalised the misclassification of those few points near the decision boundary. By penalising those misclassifications, the SVM algorithm chose a boundary which was more closely fit to the data.DS2 Repeat step 1.2 and 1.3 with DS2, justifying any change to the cross validation technique or number of folds.# load provided dataset from save fileDS2 = pd.read_csv(&#39;D2.csv&#39;, header=None)X2 = DS2.iloc[:, 0:2]y2 = DS2.iloc[:, 2]N_SAMPLES2 = DS2.shape[0]# C=10^6 to enable linear SVM to find a sufficient decision boundarysvc2linear = fit_and_plot(X2, y2, kernel=&#39;linear&#39;, C=10**6)The linear kernel was not able to select a decision boundary for this data with any lower complexity parameters attempted. But with a complexity parameter so high, conducting a full Leave-One-Out cross-validation became too computationally expensive, so I am switching here to a 5-fold cross-validation.from sklearn.model_selection import KFold# print training and test scores from CVkf = KFold(n_splits=5)cv2 = cross_validate(svc2linear, X2, y2, cv=kf.split(X2, y2), return_train_score=True)print_scores(cv2)CV train_scores: 0.55*1, 0.555*1, 0.568*1, 0.582*1, 0.59*1Mean CV train_score 0.569 CV test_scores: 0.44*1, 0.49*1, 0.51*1, 0.54*2Mean CV test_score 0.504 Pick a kernel which will improve the SVM, plot the data set and resulting decision boundary, give the performance.# fit, plot and print cv scores of SVM with RBF kernelsvc2rbf = fit_and_plot(X2, y2, kernel=&#39;rbf&#39;)cv2 = cross_validate(svc2rbf, X2, y2, cv=loo.split(X, y), return_train_score=True)print_scores(cv2)CV train_scores: 0.847*4, 0.864*4, 0.881*35, 0.898*12, 0.915*5Mean CV train_score 0.88383 CV test_scores: 0.0*12, 1.0*48Mean CV test_score 0.8 Discuss which kernel was chosen and why.I chose to use the Radial Basis kernel, because it is the latest-and-greatest standard kernel for non-linear tasks. This dataset is clearly one that requires nonlinear separation, which would be more difficult or impossible to achieve with polynomial or other kernels. RBF was able to efficiently find a reasonable decision boundary, even without tuning other hyperparameters like C.DS3 Repeat step 1.2 and 1.3 with DS3, again justifying any change to the cross validation technique or number of folds.# load provided dataset from save fileDS3 = pd.read_csv(&#39;D3.csv&#39;, header=None)X3 = DS3.iloc[:, 0:2]y3 = DS3.iloc[:, 2]N_SAMPLES3 = DS3.shape[0]svc3 = fit_and_plot(X3, y3, kernel=&#39;linear&#39;)This dataset is linearly separable without a high complexity parameter, so I am reverting back to leave-one-out CV to maximise insight into the model’s performance.cv3 = cross_validate(svc3, X3, y3, cv=loo.split(X1, y1), return_train_score=True)print_scores(cv3)CV train_scores: 0.898*54, 0.915*6Mean CV train_score 0.8997 CV test_scores: 0.0*6, 1.0*54Mean CV test_score 0.9 Pick a kernel and 2 parameters and optimize, optimize the parameters, plot again the data set and decision boundary, and give the performance.from sklearn.model_selection import GridSearchCVparams = {&#39;kernel&#39;: [&#39;sigmoid&#39;], &#39;gamma&#39;: np.linspace(0, 1, 20), &#39;coef0&#39;: np.linspace(-5, 5, 20), &#39;C&#39;: [10**x for x in [-2,-1,1,2]]}grid_search = GridSearchCV(SVC(), params, n_jobs=2).fit(X3,y3)svc3sig = fit_and_plot(X3, y3, **grid_search.best_params_)grid_search.best_params_{&#39;C&#39;: 10, &#39;coef0&#39;: -3.4210526315789473, &#39;gamma&#39;: 0.47368421052631576, &#39;kernel&#39;: &#39;sigmoid&#39;}cv3sig = cross_validate(svc3sig, X3, y3, cv=loo.split(X3, y3), return_train_score=True)print_scores(cv3sig)CV train_scores: 0.915*3, 0.95*2, 0.955*2, 0.96*129, 0.965*60, 0.97*4Mean CV train_score 0.96087 CV test_scores: 0.0*9, 1.0*191Mean CV test_score 0.955 Discuss the results of the previous step.I chose the sigmoid kernel due to the near-linear separation between the two classes of this dataset. By tuning the gamma and coef0 parameters of the sigmoid kernel, the near-linear decision boundary was able to squiggle into the small gaps between the two classes and improve the accuracy of the model.However, even with the improvement in test accuracy, I would actually prefer to keep the linear model. The linear separation between classes is quite clear, and there was no difference between training and test accuracy of the linear model. With so few observations in the dataset, the accuracy improvement of the sigmoid model is likely to just be the result of overfit to noise. I think the linear model describes the data better, and would likely end up performing better on future data." }, { "title": "NZ Housing Policy Exploration", "url": "/posts/tenancies-amendment/", "categories": "Data Science", "tags": "R, tidyverse, ggplot, time-series, forecasting", "date": "2022-05-16 00:00:00 +1200", "snippet": "In this post, I illustrate an end-to-end data science project investigating the impact of the February 2021 Residential Tenancies Amendment Bill by observing changing rental trends on the New Zealand housing market.Data SourceThe data is sourced from the NZ Tenancy Serviceswebsite.Three CSV files are provided with territorial, regional, and quarterlyaggregate bond statistics. In order to translate Location Id’s from thequarterly dataset into location names, it is also necessary to retrievedata from the Stats NZsite.Data ProcessingOne of the first data processing tasks is to ensure each data column isimported as the correct type, to ensure proper usage later in theanalysis.Some of the “.Bond” and “.Rent” columns of each dataset import ascharacter-type columns, which does not match their numeric contents. Ifound that removing the thousands-delimiting commas made converting tointeger type possible.# remove comma from numeric values and convert columnslibrary(tidyverse)q2 &amp;lt;- quarterly %&amp;gt;% replace_na(list(Location.Id=-1)) %&amp;gt;% mutate(across(str_subset(names(quarterly), regex(&quot;.Bond|(n|e).Rent|.Id&quot;)), function(x) {as.integer(gsub(&#39;,&#39;, &#39;&#39;, x))}))The next data processing step is to convert the Time Frame column intomore useful year and quarter columns. The lubridate library makesthat a simple task.I also joined the Stats NZ location ID and location name columns withthe quarterly report, so that location names could be easily included infuture analysis. It was necessary to manually add ID -99 to get thelocation name &quot;ALL&quot; to join correctly.fname &amp;lt;- &quot;statistical-area-2-2019-centroid-true&quot;fpath &amp;lt;- paste0(&quot;../data/statsnz&quot;, fname, &quot;-CSV/&quot;, fname, &quot;.csv&quot;)# import and join location namesareas &amp;lt;- read.csv(fpath)locations &amp;lt;- areas %&amp;gt;% mutate(ID = SA22019_V1_00) %&amp;gt;% mutate(Location = SA22019_V1_00_NAME) %&amp;gt;% dplyr::select(ID, Location)all_locations &amp;lt;- rbind(locations, list(-99, &#39;ALL&#39;))q.df &amp;lt;- left_join(q3, all_locations, by=c(&#39;Location.Id&#39; = &#39;ID&#39;))A similar set of steps were followed to import the regional andterritorial datasets as well, which can be seen in the Appendix.Data ExplorationThe first area of the data explored is the active bond numbers beforeand after the two-phase Bill implementation in August 2020 and February2021, searching for any patterns that may change around those dates.There does seem to be some unexpected change through 2021, whichsuggests some effect, or perhaps some external factor. Let’s check thenumber of Total and Closed Bonds as well.It seems that both Closed and Lodged Bonds follow a similar annualpattern, though with some difference toward the beginning and end of theyear. The obvious indicators here are the severe dips in April of 2020and Aug/Sep of 2021, probably related to the level 4 COVID lockdownswhich occurred around those times. It is interesting to note the firstfew months of 2022: both Closed and Lodged bond numbers are downrelative to previous years. This could indicate that there is lessmoving between residences by renters, as opposed to less residences onthe market.Let’s check the disaggregated quarterly dataset to see if any bondpatterns differ across dwelling types.Again, it seems that COVID lockdowns in Q2 2020 and Q3 2021 caused asignificant perturbation to bond filings across the board, making anyeffects from the Bill very difficult to distinguish.Let’s move to the regional dataset to search for patterns on theregional dimension.Auckland, Canterbury, Waikato, Wellington, and a few others demonstratea strong upward trend before the Bill, with a downward trend afterward.Those regions also harbor the largest cities in New Zealand, which begsthe question: is this a pattern mostly correlated with urbanenvironments? Could the trend change be an “urban flight” phenomenoncaused by COVID-19, rather than changes to rental conditions caused bythe Bill?There may be too many territories to manage in one plot, but it’s wortha try to see if any patterns can be gleaned at the territorial level. Inthe plot below, I’ve separated the data before and after Phase 1 of theBill, and smoothed each side with loess regression, to try to focus theillustration on pattern change.Analytical PlanBased on the information illuminated in the data exploration, it seemsthat the effects of the Bill have varied to some small degree acrossdwelling types and to a great degree across locations. The analyticalpath forward is not obvious, to me anyway, because of the hugeconfounding factors and inherent inability of establishing causality inobservational data. I decided to fit a series of models to the databefore the Bill’s implementation, and compare the predictions withactual data from the time after the Bill. Random Forest makes for agreat baseline, so that is attempted first. Naive and Holt-Wintersseasonal predictions are also attempted. Perhaps the most informative,due to the ease of extracting a confidence interval of prediction, is aBayesian structural time-series model. Two more ad-hoc methods ofanalysis are attempted as well: spline regression of month-over-monthchanges in data, to address autocorrelation between data points, andchangepoint detection. Hopefully, with information gathered from a fewdifferent models, the effects of the Bill become more clear.ResultsBelow, a Random Forest model trained on all regional monthly data before2021 (date, year, month and Location variables), predicts the number ofActive Bonds in each region.Prediction is generally not too far from the truth, though it variesgreatly by location, and does not capture obvious trajectories, as bestillustrated by Tasman and Waikato. Because it is the same model for alllocations, it is not able to distinguish the varying seasonality of eachlocation individually. Dunedin’s extreme seasonality may be causing somephantom seasonality in other locations. Overall RF at least gives a fairbaseline to compare further predictive models.To prevent further crossover noise between locations, all furthermodelling and analysis is conducted on Auckland alone. Auckland is themost populated region in New Zealand, so seems the natural choice.The next models that may give some insight to the effects of the 2021Bill are a naive seasonal model and, likely more effective,Double-Seasonal Holt-Winters forecasting. These models are trained ondata between the years 2000 and 2020 (or 2021 in the case of DSHW_2),and their predictions plotted alongside the actual Closed Bonds datasince 2017 below.The more sophisticated models are actually less accurate than the naivemodel! This is because they are able to pick up the existing seasonaltrends, illustrating the trend change very well. The split between theDSHW models and actual data tracks well with the Bill’s second phase inFebruary of 2021, with the late 2020 downturn almost perfectly accountedfor by seasonality.Perhaps a more informative model is attempted on the same data below: aBayesian Structural Time Series Model.The Bayesian model tracks the upward trend of previous years, butactually underestimates the Phase 1 period. The model is even bucked offfrom the actual data before the prediction period, by the massive shockof COVID-19 in early 2020.The best feature of the Bayesian model is that it also shows a 90%confidence interval. As illustrated by the gray band in the above graph,the actual number of Active Bonds in Auckland has never strayed outsidewhat could reasonably be attributed to random changes in the market.While this doesn’t confirm that the Bill had no effect on the rentalmarket, it does confirm that it did not cause a significant effect.An unaddressed issue with the modelling thus far is autocorrelation:each time-series data point is correlated with the one previous to it.Below, I attempt to address this autocorrelation. A logarithmic scale isapplied to month-over-month changes to the number of Active Bonds, andthe log results are transformed back into positive and negative numbers.By applying a linear regression to these transformed values, the sameunderlying Active Bonds patterns can be seen in a new light.Since the Bill’s implementation, and especially in recent months, thereis a negative trend in how many bonds are active in Auckland.Another model I thought might provide some insight to this data ischangepoint detection. The below plot shows the Auckland Active Bondsdata overlayed with the dates selected by several iterations of thechangepoint model, each iteration with a different number of points toidentify. Comparing the Bill’s implementation dates with the clusters ofchangepoints detected, there is again some suggestion that the Bill (orits contemporary pandemic issues) have impacted the rental market insome way.DiscussionIt is fantastic that NZ Tenancy Services provides this rental data tothe public. It is clean and thorough, and provides universal groundtruth for discussions about policy effectiveness. With finding patternsin this data being the goal of this project, it has actually been quitesatisfying to iterate on different models and methods of analysis andultimately discover the patterns presented here. In this case, however,the data is insufficient to prove any claims about the Tenancy Bill,because of the incredibly confounding factor of COVID-19, among otherthings. Causality simply cannot be proven to any degree in anobservational analysis such as this.AppendixAppendix includes all code necessary to replicate the report.# import quarterly dataset, show names and tabulate typesquarterly &amp;lt;- read.csv(&quot;../data/Detailed Quarterly Tenancy1.csv&quot;)#names(quarterly)#table(sapply(quarterly, class))# remove comma from numeric values and convert columnslibrary(tidyverse)q2 &amp;lt;- quarterly %&amp;gt;% replace_na(list(Location.Id=-1)) %&amp;gt;% mutate(across(str_subset(names(quarterly), regex(&quot;.Bond|(n|e).Rent|.Id&quot;)), function(x) {as.integer(gsub(&#39;,&#39;, &#39;&#39;, x))}))# create year and month columns from Time.Framelibrary(lubridate)q3 &amp;lt;- q2 %&amp;gt;% mutate(date = mdy_hm(Time.Frame), .before=1) %&amp;gt;% mutate(year = year(date)) %&amp;gt;% mutate(quarter = quarter(date)) %&amp;gt;% mutate(date2 = paste(year, quarter, sep=&#39;-&#39;))fname &amp;lt;- &quot;statistical-area-2-2019-centroid-true&quot;fpath &amp;lt;- paste0(&quot;../data/statsnz&quot;, fname, &quot;-CSV/&quot;, fname, &quot;.csv&quot;)# import and join location namesareas &amp;lt;- read.csv(fpath)locations &amp;lt;- areas %&amp;gt;% mutate(ID = SA22019_V1_00) %&amp;gt;% mutate(Location = SA22019_V1_00_NAME) %&amp;gt;% dplyr::select(ID, Location)all_locations &amp;lt;- rbind(locations, list(-99, &#39;ALL&#39;))q.df &amp;lt;- left_join(q3, all_locations, by=c(&#39;Location.Id&#39; = &#39;ID&#39;))# import and clean regional datasetregional &amp;lt;- read.csv(&quot;../data/rentalbond-data-regional1.csv&quot;)r2 &amp;lt;- regional %&amp;gt;% mutate(across(c(Lodged.Bonds, Closed.Bonds), function(x) {as.integer(gsub(&#39;,&#39;, &#39;&#39;, x))}))r3 &amp;lt;- r2 %&amp;gt;% mutate(date = ymd(Time.Frame)) %&amp;gt;% mutate(year = year(date)) %&amp;gt;% mutate(month = month(date))r.df &amp;lt;- r3#import and clean territorial datasettla &amp;lt;- read.csv(&quot;../data/rentalbond-data-tla1.csv&quot;)t2 &amp;lt;- tla %&amp;gt;% mutate(across(c(Lodged.Bonds, Active.Bonds), function(x) {as.integer(gsub(&#39;,&#39;, &#39;&#39;, x))}))t3 &amp;lt;- t2 %&amp;gt;% mutate(date = dmy(Time.Frame), .before=1) %&amp;gt;% mutate(year = year(date)) %&amp;gt;% mutate(month = month(date))t.df &amp;lt;- t3# prepare for plottinglibrary(ggplot2)theme_set(theme_minimal())library(viridis)colours &amp;lt;- c(&quot;#440154FF&quot;, &quot;#FDE725FF&quot;)# aggregate regional Active Bondsrplot &amp;lt;- r.df %&amp;gt;% filter(Location.Id == -99, year &amp;gt; 2016) %&amp;gt;% mutate(year = factor(year))rplot %&amp;gt;% ggplot(aes(x=month, y=Active.Bonds, group=year, colour=year)) + geom_line(size=2) + geom_point(data=filter(rplot, date==as.Date(&#39;2021-02-01&#39;) | date==as.Date(&#39;2020-08-01&#39;)), size=4, colour=&quot;black&quot;) + scale_colour_viridis_d() + guides(colour = guide_legend(reverse = T)) + scale_x_discrete(&#39;month&#39;, limits=month.abb[1:12]) + ggtitle(&quot;Active Bonds Annual Patterns&quot;)# closed and lodged regional Active Bondsrplot &amp;lt;- r.df %&amp;gt;% filter(Location == &quot;ALL&quot;, year &amp;gt; 2017) %&amp;gt;% mutate(year = factor(year)) %&amp;gt;% pivot_longer(c(Lodged.Bonds, Closed.Bonds), names_to=&quot;Bond.Type&quot;, values_to=&quot;N.Bonds&quot;)rplot %&amp;gt;% ggplot(aes(x=month, y=N.Bonds, group=year, colour=year)) + facet_grid(cols=vars(Bond.Type)) + geom_line(size=1) + geom_point(data=filter(rplot, date==as.Date(&#39;2021-02-01&#39;) | date==as.Date(&#39;2020-08-01&#39;)), size=4, colour=&quot;black&quot;) + scale_colour_viridis_d() + guides(colour = guide_legend(reverse = T)) + scale_x_discrete(&#39;month&#39;, limits=month.abb[1:12]) + ggtitle(&quot;Closed and Lodged Bonds Annual Patterns&quot;)# closed and total quarterly Active Bonds per dwelling typeqplot &amp;lt;- q.df %&amp;gt;% filter(Location == &quot;ALL&quot;, Number.Of.Beds==&quot;ALL&quot;, year &amp;gt; 2017) %&amp;gt;% mutate(year = factor(year)) %&amp;gt;% pivot_longer(c(Total.Bonds, Closed.Bonds), names_to=&quot;Bond.Type&quot;, values_to=&quot;N.Bonds&quot;)qplot %&amp;gt;% ggplot(aes(x=quarter, y=N.Bonds, group=year, colour=year)) + facet_wrap(Bond.Type~Dwelling.Type, ncol=6, scales=&quot;free_y&quot;) + geom_point(size=2) + geom_line() + scale_colour_viridis_d() + guides(colour = guide_legend(reverse = T)) + theme(axis.text.y = element_blank(), axis.title.y=element_blank()) + ggtitle(&quot;Closed and Total Bonds Annual Patterns per Dwelling Type&quot;)# active bonds by regionr.loc = r.df %&amp;gt;% filter(Location.Id &amp;gt; 0, year &amp;gt; 2016)r.loc %&amp;gt;% ggplot(aes(x=date, y=Active.Bonds)) + facet_wrap(~Location, scales=&#39;free_y&#39;, ncol=4) + geom_line(colour=colours[1]) + geom_vline(xintercept = as.Date(c(&quot;2021-02-01&quot;, &quot;2020-08-12&quot;)), linetype=2, alpha=0.5) + theme(strip.text.x = element_text(hjust = 0.1), legend.position = &quot;bottom&quot;, axis.text.y = element_blank()) + ggtitle(&#39;Active Bonds by Region 2017-2021&#39;)# active bonds by territoryt.loc = t.df %&amp;gt;% filter(Location.Id &amp;gt; 0, year &amp;gt; 2016)t.loc %&amp;gt;% ggplot(aes(x=date, y=Active.Bonds)) + facet_wrap(~Location, scales=&#39;free_y&#39;, ncol=6) + geom_smooth(data=filter(t.loc, date &amp;lt;= as.Date(&quot;2020-08-12&quot;)), span=0.99, colour=colours[2]) + geom_smooth(data=filter(t.loc, date &amp;gt; as.Date(&quot;2020-08-01&quot;)), span=0.99, colour=colours[1]) + geom_vline(xintercept = as.Date(c(&quot;2021-02-01&quot;, &quot;2020-08-12&quot;)), linetype=2, alpha=0.5) + theme(strip.text.x = element_text(hjust = 0.1), axis.text = element_blank()) + ggtitle(&#39;Active Bonds by Territory&#39;)# train and plot predictions of Random Forestlibrary(ranger)fdata &amp;lt;- r.df %&amp;gt;% filter(Location.Id &amp;gt; 0) %&amp;gt;% dplyr::select(date, year, month, Location, Active.Bonds)train &amp;lt;- fdata %&amp;gt;% filter(year &amp;gt;= 2000, date &amp;lt;= as.Date(&quot;2020-08-12&quot;))test &amp;lt;- fdata %&amp;gt;% filter(date &amp;gt;= as.Date(&quot;2020-08-12&quot;))forest &amp;lt;- ranger(Active.Bonds ~ year + month + Location, data=train, num.trees=100, mtry=3)p &amp;lt;- predict(forest, test)trainplot &amp;lt;- filter(train, year&amp;gt;2017) %&amp;gt;% mutate(RF.Prediction=NA)testplot &amp;lt;- cbind(test, RF.Prediction=p$predictions) fplot &amp;lt;- rbind(trainplot, testplot) %&amp;gt;% pivot_longer(c(Active.Bonds, RF.Prediction), names_to=&#39;source&#39;, values_to=&#39;value&#39;)fplot %&amp;gt;% ggplot(aes(x=date, y=value, colour=source)) + facet_wrap(~Location, scales=&#39;free_y&#39;, ncol=4) + geom_line(size=1) + geom_vline(xintercept = as.Date(c(&quot;2021-02-01&quot;,&quot;2020-08-12&quot;)), linetype=&#39;dashed&#39;, alpha=0.5) + scale_colour_viridis_d() + theme(strip.text.x = element_text(hjust = 0.1), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;, axis.title.y=element_blank()) + ggtitle(&#39;Active Bonds vs Random Forest Prediction 2020-2022&#39;)# fit and plot naive and double-seasonal Holt-Winters forecastinglibrary(forecast)year_start = 2000timesdf &amp;lt;- r.df %&amp;gt;% filter(Location.Id == 2, year &amp;gt;= year_start) %&amp;gt;% arrange(year, month) %&amp;gt;% dplyr::select(date, Active.Bonds)timesr = ts(timesdf$Active.Bonds, start=year_start, deltat=1/12)training &amp;lt;- window(timesr, start=c(year_start, 1), end=c(2020, 7))validation &amp;lt;- window(timesr, start=c(2020, 8))train_covid &amp;lt;- window(timesr, end=c(2021, 1))valid_covid &amp;lt;- window(timesr, start=c(2021, 2))naive = snaive(training, h=length(validation), lambda=&#39;auto&#39;)dshw &amp;lt;- dshw(training, period1=4, period2=12, h=length(validation))dshw_covid &amp;lt;- dshw(train_covid, period1=4, period2=12, h=length(valid_covid))naive.df &amp;lt;- data.frame(date=zoo::as.Date(time(naive$mean)), B_Naive=naive$mean)dshw.df &amp;lt;- data.frame(date=zoo::as.Date(time(dshw$mean)), DSHW_1=dshw$mean)dshw_covid.df &amp;lt;- data.frame(date=zoo::as.Date(time(dshw_covid$mean)), DSHW_2=dshw_covid$mean)timesplot &amp;lt;- r.df %&amp;gt;% filter(Location.Id == 2, year &amp;gt; 2017) %&amp;gt;% arrange(year, month) %&amp;gt;% left_join(naive.df, by=(date=&#39;date&#39;)) %&amp;gt;% left_join(dshw.df, by=(date=&#39;date&#39;)) %&amp;gt;% left_join(dshw_covid.df, by=(date=&#39;date&#39;)) %&amp;gt;% pivot_longer(c(B_Naive, DSHW_1, DSHW_2), names_to=&#39;Model&#39;, values_to=&#39;Active.bonds&#39;)timesplot %&amp;gt;% ggplot(aes(x=date, y=Active.bonds, colour=Model)) + geom_line(aes(x=date, y=Active.Bonds, colour=&#39;Actual&#39;), size=1) + geom_line(linetype=2, size=1) + geom_vline(xintercept = as.Date(c(&quot;2021-02-01&quot;,&quot;2020-08-01&quot;)), linetype=&quot;dashed&quot;, alpha=0.5) + scale_colour_viridis_d() + theme(legend.position = &#39;bottom&#39;) + ggtitle(&#39;Auckland Active Bonds - Seasonal Predictions&#39;)# fit and plot Bayesian Structural Time Series Modellibrary(bsts)Y &amp;lt;- trainingy &amp;lt;- log10(Y)ss &amp;lt;- AddLocalLinearTrend(list(), y)ss &amp;lt;- AddSeasonal(ss, y, nseasons=4)ss &amp;lt;- AddSeasonal(ss, y, nseasons=12)bsts.model &amp;lt;- bsts(y, state.specification=ss, niter=500)burn &amp;lt;- SuggestBurn(0.1, bsts.model)bsts.p &amp;lt;- predict.bsts(bsts.model, horizon=length(validation), burn=burn, quantiles=c(0.05, 0.95))d2 &amp;lt;- data.frame(c(10^as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])+y), 10^as.numeric(bsts.p$mean)), as.numeric(timesr), as.Date(time(timesr)))names(d2) &amp;lt;- c(&quot;Fitted&quot;, &quot;Actual&quot;, &quot;Date&quot;)post.interval &amp;lt;- cbind.data.frame( 10^as.numeric(bsts.p$interval[1,]), 10^as.numeric(bsts.p$interval[2,]), subset(d2, Date&amp;gt;=as.Date(&#39;2020-08-01&#39;))$Date)names(post.interval) &amp;lt;- c(&quot;Lower&quot;, &quot;Upper&quot;, &quot;Date&quot;)### Join intervals to the forecastd3 &amp;lt;- left_join(d2, post.interval, by=&quot;Date&quot;)d3.plot &amp;lt;- subset(d3, year(Date) &amp;gt; 2017)### Plot actual versus predicted with credible intervals for the holdout periodggplot(data=d3.plot, aes(x=Date)) + geom_ribbon(aes(ymin=Lower, ymax=Upper), fill=&quot;grey&quot;, alpha=0.5) + geom_line(aes(y=Actual, colour = &quot;Actual&quot;), size=1) + geom_line(aes(y=Fitted, colour = &quot;Fitted&quot;), size=1) + scale_colour_viridis_d() + ylab(&quot;Active.Bonds&quot;) + xlab(&quot;date&quot;) + geom_vline(xintercept=as.Date(c(&quot;2021-02-01&quot;,&quot;2020-08-01&quot;)), linetype=2, alpha=0.5) + theme(legend.title = element_blank(), legend.position = &#39;bottom&#39;) + ggtitle(&#39;Auckland Active Bonds - Bayesian Structural Time Series Prediction&#39;)# month over month distribution analysisy = 2018mom.df &amp;lt;- t.df %&amp;gt;% filter(Location.Id &amp;gt; 0, year &amp;gt;= y) %&amp;gt;% arrange(Location, date) %&amp;gt;% mutate(MoM.Active = c(0, diff(Active.Bonds))) %&amp;gt;% filter(year &amp;gt; y) %&amp;gt;% # diff will cross locations, so filter cross points dplyr::select(date, Location, MoM.Active, year, month) %&amp;gt;% mutate(logMoM=ifelse(MoM.Active &amp;gt; 0, log(MoM.Active), -log(-MoM.Active))) %&amp;gt;% mutate(logMoM=ifelse(is.infinite(logMoM), 0, logMoM))mom.before &amp;lt;- mom.df %&amp;gt;% filter(date &amp;lt;= as.Date(&quot;2020-08-01&quot;))mom.mid &amp;lt;- mom.df %&amp;gt;% filter(date &amp;lt;= as.Date(&quot;2021-02-01&quot;), date &amp;gt;= as.Date(&quot;2020-08-01&quot;))mom.after &amp;lt;- mom.df %&amp;gt;% filter(date &amp;gt;= as.Date(&quot;2021-02-01&quot;))mom.df %&amp;gt;% ggplot(aes(x=date, y=logMoM)) + geom_point(alpha=0.2, colour=colours[1]) + geom_smooth(method=&quot;lm&quot;, data=mom.before, colour=colours[2]) + geom_smooth(method=&quot;lm&quot;, data=mom.mid, colour=colours[2]) + geom_smooth(method=&quot;lm&quot;, data=mom.after, colour=colours[2]) + scale_colour_viridis_d() + geom_vline(xintercept=as.Date(c(&quot;2021-02-01&quot;,&quot;2020-08-01&quot;)), linetype=2, alpha=0.5) + theme(legend.title = element_blank(), legend.position = &#39;bottom&#39;) + ggtitle(&#39;Auckland Active Bonds - Month over Month changes (+/- corrected log)&#39;)library(segmented)cp.df &amp;lt;- r.df %&amp;gt;% filter(Location.Id == 2, year &amp;gt; 2018) %&amp;gt;% arrange(year, month)yy &amp;lt;- cp.df$Active.Bondscp1 &amp;lt;- cp.df$date[as.integer(segmented(yy, npsi=1)$psi[,&quot;Est.&quot;]*nrow(cp.df))]cp2 &amp;lt;- cp.df$date[as.integer(segmented(yy, npsi=2)$psi[,&quot;Est.&quot;]*nrow(cp.df))]cp3 &amp;lt;- cp.df$date[as.integer(segmented(yy, npsi=3)$psi[,&quot;Est.&quot;]*nrow(cp.df))]cp4 &amp;lt;- cp.df$date[as.integer(segmented(yy, npsi=4)$psi[,&quot;Est.&quot;]*nrow(cp.df))]cp5 &amp;lt;- cp.df$date[as.integer(segmented(yy, npsi=5)$psi[,&quot;Est.&quot;]*nrow(cp.df))]ggplot(data=cp.df, aes(x=date, y=Active.Bonds)) + geom_line(colour=colours[1]) + geom_vline(xintercept=as.Date(c(&quot;2021-02-01&quot;,&quot;2020-08-12&quot;)), linetype=2, alpha=0.5) + geom_vline(xintercept = cp1, colour=colours[2], size=1) + geom_vline(xintercept = cp2, colour=colours[2], size=1) + geom_vline(xintercept = cp3, colour=colours[2], size=1) + geom_vline(xintercept = cp4, colour=colours[2], size=1) + geom_vline(xintercept = cp5, colour=colours[2], size=1) + ggtitle(&#39;Auckland Active Bonds - Changepoint Detection&#39;)" }, { "title": "Data Visualisation in base R", "url": "/posts/rgraphics/", "categories": "Data Visualisation", "tags": "R, graphics, visualisation", "date": "2022-05-12 00:00:00 +1200", "snippet": "In this post, I play with various plotting and visualisation options provided by base R, with a strong focus on small details.Question 1a)# compute x and y for full distributionx = seq(-4, 4, by=0.025)y = dt(x, 20)# get x and y of rejection regionrej_x = x[x &amp;gt;= 1.725]rej_y = tail(y, length(rej_x))# create plotplot.new()plot.window(xlim=c(-4, 4), ylim=c(0, 0.4), bty=&#39;n&#39;, yaxs=&#39;i&#39;)title(main=&#39;t(df=20) distribution&#39;, xlab=&#39;test-statistic&#39;)# draw and fill distributionlines(x, y)polygon(x, y, border=NA, col=hcl(120, 50, alpha=0.5))# draw and fill rejection regionlines(x=rep(1.725, 2), y=c(0, 0.4), lty=&#39;dashed&#39;)polygon(c(1.725, rej_x), c(0, rej_y), border=NA, col=hcl(270, 230))# add x axis and textaxis(1)mtext(&#39;p = 0.05&#39;, side=3)text(2.8, 0.1, &#39;P(X &amp;gt; 1.725) = p&#39;)b)# draw and fill a single rejection regiondraw_rejection &amp;lt;- function(x, y, p, border, onesided) { # dashed line at x lines(x=rep(border, 2), y=c(0, 1), lty=&#39;dashed&#39;) # fill between border and tail polygon(c(border, x), c(0, y), border=NA, col=hcl(270, 230)) # add detail text text(border*1.08, max(y)*1.1, adj=ifelse(border &amp;lt; 0, 1, 0), paste0(&#39;P(X &#39;, ifelse(border &amp;lt; 0, &#39;&amp;lt;&#39;, &#39;&amp;gt;&#39;), &#39; &#39;, border, &#39;) = p&#39;, ifelse(onesided, &#39;&#39;, &#39;/2&#39;)))}f &amp;lt;- function(df, p, onesided) { # choose int x range limits based on some minimum p xrange = round(qt(0.000125, df, lower.tail=F)) # compute x and y for full distribution x = seq(-xrange, xrange, by=0.01) y = dt(x, df) # create plot plot.new() plot.window(xlim=c(-xrange, xrange), ylim=c(0, max(y)), bty=&#39;n&#39;, yaxs=&#39;i&#39;) title(main=paste0(&#39;t(df=&#39;, df, &#39;) distribution&#39;), xlab=&#39;test-statistic&#39;) mtext(paste(&#39;p =&#39;, p), side=3) axis(1) # draw and fill distribution lines(x, y) polygon(x, y, border=NA, col=hcl(120, 50, alpha=0.5)) # if not onesided, draw the left rejection region if (!onesided) { p = p/2 border = round(qt(p, df), 3) rej_x = x[x &amp;lt;= border] rej_y = head(y, length(rej_x)) draw_rejection(rej_x, rej_y, p, border, onesided) } # always draw the right rejection region border = round(qt(p, df, lower.tail=F), 3) rej_x = x[x &amp;gt;= border] rej_y = tail(y, length(rej_x)) draw_rejection(rej_x, rej_y, p, border, onesided)}f(20, 0.05, TRUE)f(10, 0.03, FALSE)c)# for reproducibilityset.seed(400)# sample 30 from N(3,2) and compute t-statisticsn = 30sample = rnorm(n, mean=3, sd=2)means = seq(2, 4, by=0.5)tstats = (mean(sample) - means) / (sd(sample) / sqrt(n))# plot t-distribution with rejection regionsf(29, 0.05, F)# overlay t-stat lines and labelsfor (i in seq_along(tstats)) { lines(x=rep(tstats[i], 2), y=seq(0, 1)) text(x=tstats[i], y=0.3, srt=-90, pos=4, labels=bquote(mu ~ &#39;=&#39; ~ .(means[i])))}As illustrated above, μ = 2 and μ = 4 fall in the rejection region,indicating that the null hypothesis is rejected and the alternative isaccepted given those mean values. The alternative hypothesis is notaccepted for the other tested values of μ.Question 2a)# read csv and separate year from monthimports &amp;lt;- read.csv(&#39;imports-by-country.csv&#39;)imports$year = imports$yearmonth %/% 100imports$month = as.integer(imports$yearmonth %% 100)imports = subset(imports, select=-c(yearmonth))head(imports)## country value year month## 1 Afghanistan 60538 2000 1## 2 Afghanistan 21641 2000 2## 3 Afghanistan 28603 2000 3## 4 Afghanistan 34781 2000 4## 5 Afghanistan 3130 2000 5## 6 Afghanistan 11199 2000 6# list the top three countries by total value of importscountry_sums &amp;lt;- aggregate(value ~ country, imports, sum)sorted_country_sums &amp;lt;- country_sums[order(-country_sums$value),]top_three_countries &amp;lt;- head(sorted_country_sums, 3)top_three_countries## country value## 43 China, People&#39;s Republic of 157224053406## 12 Australia 153492831803## 233 United States of America 105147208043b)# draw a pie chart of total imports from each countrywith(sorted_country_sums, pie(value, labels=country, main=&#39;Proportion of imports to NZ since 2000&#39;))There are too many countries listed in this visualisation, so I wouldprefer to bin the smallest into an “other” group or narrow the range ofcountries listed.The default colors used in this visualisation repeat, which may causethe viewer to wrongly associate unrelated countries.c)# get top 15 countries by average annual importsannual_imports &amp;lt;- aggregate(value ~ country + year, imports, sum)avg_annual_imports &amp;lt;- aggregate(value ~ country, annual_imports, mean)sorted_avg_imports &amp;lt;- avg_annual_imports[order(-avg_annual_imports$value),]# convert values to billions NZD and showsorted_avg_imports$value &amp;lt;- round(sorted_avg_imports$value / 10^9, 1)top_annual &amp;lt;- head(sorted_avg_imports, 15)top_annual## country value## 43 China, People&#39;s Republic of 7.1## 12 Australia 7.0## 233 United States of America 4.8## 110 Japan 3.4## 85 Germany 2.1## 115 Korea, Republic of 1.6## 192 Singapore 1.5## 217 Thailand 1.5## 130 Malaysia 1.4## 231 United Kingdom 1.2## 230 United Arab Emirates 1.0## 77 France 0.9## 108 Italy 0.9## 214 Taiwan 0.8## 103 Indonesia 0.7# remove &#39;republic of&#39; from country namescnames &amp;lt;- sapply(strsplit(top_annual$country, &#39;,&#39;), `[`, 1)# barplot the top 15 countriespar(mar=c(11,4,1,0))mybar &amp;lt;- barplot(top_annual$value, ylim=c(0, 8), names.arg=cnames, las=2)text(mybar, top_annual$value+0.4, top_annual$value)title(ylab=&#39;Avg Annual Imports (Billions NZD)&#39;, adj=1)d)# ---- DATA PROCESSING ----# get monthly imports of top 11 by avg annual importstop_eleven &amp;lt;- head(sorted_avg_imports, 11)top_monthly &amp;lt;- subset(imports, country %in% top_eleven$country)# sum up monthly imports of all other countriesall_others_monthly &amp;lt;- subset(imports, !(country %in% top_eleven$country))other_monthly &amp;lt;- aggregate(value ~ year + month, all_others_monthly, sum)other_monthly$country &amp;lt;- &#39;Other&#39;# combine top 11 with &#39;other&#39; and sort by time/datemonthly &amp;lt;- rbind(top_monthly, other_monthly)monthly$time &amp;lt;- as.Date(with(monthly, paste(year, month, &#39;01&#39;, sep=&#39;-&#39;)))monthly &amp;lt;- monthly[order(monthly$time),]# convert monthly imports to billions and show sums to confirmmonthly$value &amp;lt;- round(monthly$value / 10^9, 2)aggregate(value ~ country, monthly, sum)## country value## 1 Australia 153.48## 2 China, People&#39;s Republic of 157.23## 3 Germany 46.17## 4 Japan 74.43## 5 Korea, Republic of 34.72## 6 Malaysia 30.59## 7 Other 251.72## 8 Singapore 33.33## 9 Thailand 32.37## 10 United Arab Emirates 21.12## 11 United Kingdom 26.91## 12 United States of America 105.07# ---- PLOTTING ----# merge sorted country names to create palettecountries &amp;lt;- c(&#39;Other&#39;, top_annual$country)cols &amp;lt;- rainbow(length(countries))# create plotplot.new()par(mar=c(2,4,1,0))with(monthly, plot.window(xlim=c(min(time), max(time)), ylim=c(0, max(value)+0.6)))box()# plot a line for each country&#39;s monthly importsfor (i in seq_along(countries)) { country_imports &amp;lt;- monthly[monthly$country==countries[i],] with(country_imports, lines(time, value, col=cols[i]))}# add title and axestitle(main=&#39;Top 15 Importers to NZ: 2000-2021&#39;, ylab=&#39;Monthly Import value (billions NZD)&#39;)axis.Date(1, monthly$time)axis(2, at=seq(0, 2, by=0.4), las=2)legend(&#39;topleft&#39;, legend=countries, col=cols, lty=1, lwd=3, ncol=2)e)# drop &#39;Other&#39; from countriesm &amp;lt;- monthly[monthly$country!=&#39;Other&#39;,]co &amp;lt;- top_eleven$country# create year range and color paletteyears &amp;lt;- 2000:2021legend_inds &amp;lt;- seq(1, length(years), by=3)angles &amp;lt;- (seq_along(years)-1)/length(years)cols &amp;lt;- hcl(h=angles*300, c=60, l=70)# create layout and loop through countriespar(mfrow=c(6,2), mar=rep(0,4), oma=c(2,2,4,2), cex.main=1.5)for (i in seq_along(co)) { # get monthly imports of this country c.df &amp;lt;- m[m$country == co[i],] # create new plot plot.new() plot.window(xlim=c(1, 12), ylim=c(min(c.df$value), max(c.df$value))) box() # draw a line for each year of imports for (j in seq_along(years)) with(c.df[c.df$year == years[j],], lines(month, value, col=cols[j], lwd=2)) # add title and draw axis if left/right/bottom edge title(main=co[i], line=-1.5) if(i %% 2 == 0) axis(4) else axis(2) if(i &amp;gt; length(co)-2) axis(1, at=1:12, labels=month.abb[1:12])}# add legend in final plot space and top titleplot.new()legend(&#39;center&#39;, legend=years[legend_inds], col=cols[legend_inds], cex=1.5, ncol=2, lty=1, lwd=5, bty=&#39;n&#39;)par(cex.main=2)title(main=&#39;Monthly Imports by Country (in billions NZD)&#39;, outer=T)f)Based on the above figure, imports from Australia have a clear seasonalpattern. Each year consistently bottoms out in January and climbs to apeak in December, likely due to the timing of winter holidays around theNew Year.The above plots also illuminate steadily increasing imports from nationslike China and Thailand. The warm hues of earlier years are consistentlylower than the cooler hues of later years’ lines, reflecting a steadyincrease in imports each year." }, { "title": "Naive Bayes Classification", "url": "/posts/naive-bayes/", "categories": "Machine Learning", "tags": "python, sklearn, machine-learning, naive-bayes", "date": "2022-05-06 00:00:00 +1200", "snippet": "This post details the development process for my submission to the Point-of-Interest Categorization competition on Kaggle.MotivationIn order to match the benchmark model performance initially, some default parameters had to be modified straightaway. I chose to ignore prior probabilities in the Naive Bayes’ probability calculation on suspicion that the training data’s category distribution was imbalanced. I also chose to tune the model’s smoothing parameter, as that seemed a necessary change to account for words with zero count. After making those two changes, I was able to closely match the benchmark model’s performance. Further improvement to prediction scores then needed to be achieved by other means.Task 1I chose to first attempt to improve prediction by excluding english stop words from the word frequency vector. I also attempted to include bigrams in the vector, and tried turning off the default inverse document frequency calculation executed by the term frequency utility provided by sklearn. I also switched to the Rennie et al. (2003) version of Multinomial Naive Bayes model to try to address the issues illuminated in that paper.Task 2I chose to add the name and mean_checkin_time attributes to the model to try to improve prediction scores. Some establishment names explicitly include class information, so the benefit of including that attribute is obvious. I chose to represent that attribute as a bag of words in the same or similar way as the review attribute. I suspected that information about the time spent checking in to the establishment may be helpful as well, as some categories may be associated with longer or shorter checkins. The only other attributes available in the dataset are latitude and longitude, which do not seem very informative since different types of businesses can be mixed in the same area. There could be some association between output class and location due to local tradition or zoning policy, but it would surely require some clever preprocessing for the Naive Bayes model to discover those associations.Data Representation and PreprocessingEach observation is initially represented by a term frequency vector, constructed from the review attribute. The default behavior of sklearn’s TfidfTransformer is to incorporate the inverse document frequency in each term frequency. In Task 1, that default behavior is turned off, so basic term frequency is used. English stop words are removed from the term frequency vector in both Task 1 and 2. Bigrams from the review feature are unsuccessfully included in Task 1, but somewhat successfully included in Task 2, with the difference in effect probably due to the additional maximum limit placed on the number of features in Task 2.ImplementationInitial implementation focused on boilerplate setup and matching prediction performance with the benchmark level of about 87%. This was achieved by removing prior probabilities from prediction, and tuning the smoothing parameter.Task 1Though many changes were attempted in Task 1, the real improvement in prediction came from the combination of removing english stop words and skipping the inverse document frequency calculation. These changes, in concert with the use of Rennie et al. (2003) model, led to a consistent prediction accuracy of almost 90%. I believe the removal of stop words and use of simpler term frequency vectors enabled the more robust ComplementNB model to resist bias to the training data.Task 2Adding the name and mean_checkin_time attributes in Task 2 alone led to a small improvement in prediction accuracy. When considering the addition of more term frequency features, it occurred to me that there may be thousands of features already in the model that were not very informative. Adding the max_features argument to the Naive Bayes model, in combination with the additional attributes and re-tuning of existing hyperparameters, led to a significant improvement over the benchmark model.Evaluation ProcedureAll models were evaluated and their hyperparameters selected by 5-fold cross-validation. I decided that cross-validation was necessary to maximize the number of observations available for training, since only a few thousand observations were available in total. Fewer cross-validation folds were used for intermediate exploration, as computation time of exhaustive grid search over many hyperparameters became an issue.Validation ResultsThe validation accuracy of each model is listed below. Again, this was calculated using 5-fold cross-validation to maximize the exposure of each model to the small training set. Task 0 (benchmark): 88.5% Task 1 (optimised): 89.4% Task 2 (additional attributes): 91.4%CodePart 0a - Setupimport osimport numpy as npimport pandas as pdfrom pathlib import Pathfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import GridSearchCVfrom sklearn.naive_bayes import ComplementNBfrom sklearn.compose import ColumnTransformerfrom sklearn.preprocessing import MinMaxScalernp.random.seed(12345678)PATH_ROOT = Path(os.getcwd())train_file = os.path.join(PATH_ROOT, &#39;train.csv&#39;)test_file = os.path.join(PATH_ROOT, &#39;test.csv&#39;)benchmark_file = os.path.join(PATH_ROOT, &#39;benchmark_predict.csv&#39;)improved_file = os.path.join(PATH_ROOT, &#39;improved_predict.csv&#39;)plus_attr_file = os.path.join(PATH_ROOT, &#39;plus_attr_predict.csv&#39;)train = pd.read_csv(train_file)X_train_review = train[&#39;review&#39;]y_train = train[&#39;category&#39;]test = pd.read_csv(test_file)X_test_review = test[&#39;review&#39;]X_test_id = test[&#39;ID&#39;]Part 0b - Replicate Benchmark Model# Replicate Benchmark Modelpipe = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;tfidf&#39;, TfidfTransformer()), (&#39;clf&#39;, MultinomialNB()),])bench_params = { &#39;clf__alpha&#39;: np.arange(0.2, 0, -0.05), &#39;clf__fit_prior&#39;: [True, False],}bench_cv = GridSearchCV(pipe, bench_params, n_jobs=-1)bench_model = bench_cv.fit(X_train_review, y_train)print(&quot;Best validation score: %r&quot; % (bench_model.best_score_))print(&quot;----- Achieved with -------&quot;)for name in sorted(bench_params.keys()): print(&quot;%s: %r&quot; % (name, bench_model.best_params_[name]))Best validation score: 0.885144675049235----- Achieved with -------clf__alpha: 0.10000000000000003clf__fit_prior: False# write predictions to filey_test_pred = pd.Series(bench_model.predict(X_test_review), name=&#39;category&#39;)submission = pd.concat([X_test_id, y_test_pred], axis=1)submission.to_csv(benchmark_file, index=False)Part 1 - Improve based on Review only# Improve Benchmark Model with complement model and new paramsimproved_pipe = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;tfidf&#39;, TfidfTransformer()), # IMPROVEMENT use model from Rennie et al. 2003 (&#39;clf&#39;, ComplementNB()),])improved_params = { # IMPROVEMENT params &#39;vect__stop_words&#39;: [&#39;english&#39;], &#39;vect__ngram_range&#39;: [(1,1), (1,2)], &#39;tfidf__use_idf&#39;: [True, False], # BENCHMARK params &#39;clf__alpha&#39;: [0.1], # fit_prior unneccessary - only applies to edge cases in ComplementNB}improved_cv = GridSearchCV(improved_pipe, improved_params, cv=5, n_jobs=-1)improved_model = improved_cv.fit(X_train_review, y_train)print(&quot;Improvement over benchmark: %r&quot; %(improved_model.best_score_ - bench_model.best_score_))print(&quot;Best validation score: %r&quot; % (improved_model.best_score_))print(&quot;----- Achieved with -------&quot;)for name in sorted(improved_params.keys()): print(&quot;%s: %r&quot; % (name, improved_model.best_params_[name]))Improvement over benchmark: 0.008695046205120405Best validation score: 0.8938397212543554----- Achieved with -------clf__alpha: 0.1tfidf__use_idf: Falsevect__ngram_range: (1, 1)vect__stop_words: &#39;english&#39;# write predictions to filey_test_pred = pd.Series(improved_model.predict(X_test_review), name=&#39;category&#39;)submission = pd.concat([X_test_id, y_test_pred], axis=1)submission.to_csv(improved_file, index=False)Part 2# Improve model with additional attributes# Add &#39;name&#39; attr and use the same BoW preprocess as &#39;review&#39;words_attrs = [&#39;review&#39;, &#39;name&#39;]words_pipe = Pipeline([ (&#39;vect&#39;, CountVectorizer()), (&#39;tfidf&#39;, TfidfTransformer()),])# Add mean checkin time without any preprocessingnum_attrs = [&#39;mean_checkin_time&#39;]preprocessor = ColumnTransformer([ (&#39;review&#39;, words_pipe, &#39;review&#39;), (&#39;name&#39;, words_pipe, &#39;name&#39;), (&#39;num&#39;, &#39;passthrough&#39;, num_attrs),])best_pipe = Pipeline([ (&#39;pre&#39;, preprocessor), (&#39;clf&#39;, ComplementNB()),])best_params = { # TASK 2 IMPROVEMENT params &#39;pre__review__vect__max_features&#39;: np.arange(3000, 5000, 500), # TASK 1 IMPROVEMENT params &#39;pre__review__vect__stop_words&#39;: [&#39;english&#39;], # , None], &#39;pre__review__vect__ngram_range&#39;: [(1,2)], #, (1,1)], #&#39;pre__name__vect__ngram_range&#39;: [(1,1), (1,2)], #&#39;pre__review__tfidf__use_idf&#39;: [False], #, True], &#39;pre__name__tfidf__use_idf&#39;: [False], #, True], # BENCHMARK params &#39;clf__alpha&#39;: np.arange(0.34, 0.25, -0.02), #np.arange(0.3, 0.22, -0.02), &#39;clf__fit_prior&#39;: [True]#, False],}best_cv = GridSearchCV(best_pipe, best_params, cv=5, n_jobs=-1)best_model = best_cv.fit(train, y_train)print(&quot;Improvement over benchmark: %r&quot; %(best_model.best_score_ - bench_model.best_score_))print(&quot;Improvement over Part 1: %r&quot; %(best_model.best_score_ - improved_model.best_score_))print(&quot;Best validation score: %r&quot; % (best_model.best_score_))print(&quot;----- Achieved with -------&quot;)for name in sorted(best_params.keys()): print(&quot;%s: %r&quot; % (name, best_model.best_params_[name]))Improvement over benchmark: 0.028885320405999004Improvement over Part 1: 0.0201902742008786Best validation score: 0.914029995455234----- Achieved with -------clf__alpha: 0.32clf__fit_prior: Truepre__name__tfidf__use_idf: Falsepre__review__vect__max_features: 3500pre__review__vect__ngram_range: (1, 2)pre__review__vect__stop_words: &#39;english&#39;# write predictions to filey_test_pred = pd.Series(best_model.predict(test), name=&#39;category&#39;)submission = pd.concat([X_test_id, y_test_pred], axis=1)submission.to_csv(plus_attr_file, index=False)" }, { "title": "Decision Tree Practice", "url": "/posts/decision-tree/", "categories": "Machine Learning", "tags": "python, sklearn, machine-learning, decision-tree", "date": "2022-05-06 00:00:00 +1200", "snippet": "In this post, I compare 3 decision tree models pruned with different techniques. Analysis is conducted using 3 separate datasets to show how tree depth interacts with the dimensions of the dataset.import osimport pandas as pdimport numpy as npfrom pathlib import Pathfrom sklearn.model_selection import train_test_splitpd.set_option(&#39;mode.chained_assignment&#39;,None)PATH_ROOT = Path(os.getcwd())RANDOM_STATE = 123456np.random.seed(RANDOM_STATE)Task 1 - Data PreprocessingCode and ResultsPreprocessing - arrhythmia.csvheart = pd.read_csv(os.path.join(PATH_ROOT, &#39;arrhythmia.csv&#39;), skipinitialspace=True, na_values=&#39;?&#39;)heart_nulls = heart.isnull().sum()print(&#39;Raw arrhythmia data shape and # of nulls&#39;)print(heart.shape)print(heart_nulls[heart_nulls &amp;gt; 0])heart2 = heart.drop(&#39;J&#39;, axis=1)# --- separate training and test setsheart_x = heart2.drop(&#39;class&#39;, axis=1)heart_y = heart2.loc[:, &#39;class&#39;]h_train_x, h_test_x, h_train_y, h_test_y = train_test_split(heart_x, heart_y, test_size=0.2, random_state=RANDOM_STATE)# Interpolate training and testing data separatelyfor df in [h_train_x, h_test_x]: df.interpolate(method=&#39;linear&#39;, inplace=True) df_nulls = df.isnull().sum() print(&#39;\\n&#39;+&#39;Preprocessed data shape and # of nulls:&#39;) print(df.shape) print(df_nulls[df_nulls &amp;gt; 0])Raw arrhythmia data shape and # of nulls(452, 280)T 8P 22QRST 1J 376heartrate 1dtype: int64Preprocessed data shape and # of nulls:(361, 278)Series([], dtype: int64)Preprocessed data shape and # of nulls:(91, 278)Series([], dtype: int64)Preprocessing - BCP.csvbcp = pd.read_csv(os.path.join(PATH_ROOT, &#39;BCP.csv&#39;), dtype=np.int64)bcp_nulls = bcp.isnull().sum()print(bcp.shape)print(bcp_nulls[bcp_nulls &amp;gt; 0])print(bcp.columns)(683, 11)Series([], dtype: int64)Index([&#39;Sample code number&#39;, &#39;Clump Thickness&#39;, &#39;Uniformity of Cell Size&#39;, &#39;Uniformity of Cell Shape&#39;, &#39;Marginal Adhesion&#39;, &#39;Single Epithelial Cell Size&#39;, &#39;Bare Nuclei&#39;, &#39;Bland Chromatin&#39;, &#39;Normal Nucleoli&#39;, &#39;Mitoses&#39;, &#39;Class&#39;], dtype=&#39;object&#39;)Preprocessing - website-phishing.csvwebsite = pd.read_csv(os.path.join(PATH_ROOT, &#39;website-phishing.csv&#39;), dtype=np.int64)website_nulls = website.isnull().sum()print(website.shape)print(website_nulls[website_nulls &amp;gt; 0])website.columns = website.columns.str.strip()print(website.columns)(11055, 31)Series([], dtype: int64)Index([&#39;having_IP_Address&#39;, &#39;URL_Length&#39;, &#39;Shortining_Service&#39;, &#39;having_At_Symbol&#39;, &#39;double_slash_redirecting&#39;, &#39;Prefix_Suffix&#39;, &#39;having_Sub_Domain&#39;, &#39;SSLfinal_State&#39;, &#39;Domain_registeration_length&#39;, &#39;Favicon&#39;, &#39;port&#39;, &#39;HTTPS_token&#39;, &#39;Request_URL&#39;, &#39;URL_of_Anchor&#39;, &#39;Links_in_tags&#39;, &#39;SFH&#39;, &#39;Submitting_to_email&#39;, &#39;Abnormal_URL&#39;, &#39;Redirect&#39;, &#39;on_mouseover&#39;, &#39;RightClick&#39;, &#39;popUpWidnow&#39;, &#39;Iframe&#39;, &#39;age_of_domain&#39;, &#39;DNSRecord&#39;, &#39;web_traffic&#39;, &#39;Page_Rank&#39;, &#39;Google_Index&#39;, &#39;Links_pointing_to_page&#39;, &#39;Statistical_report&#39;, &#39;Class&#39;], dtype=&#39;object&#39;)Discussionarrhythmia I opened the csv data source in a text editor to get an idea of the raw data before attempting an import. Each column looked to be numeric, but I noticed some ? characters likely meant to denote missing values. I imported specifying the na_values argument to capture those question marks as missing values, and the skipinitialspace argument to eliminate the leading space in each column header. I then retrieved which attributes were missing, and how many values each of those attributes were missing. The J attribute was missing from the vast majority of rows, so it seemed best to simply drop that attribute entirely, as it most likely would not provide our models with any information. I judged it best to linear-interpolate the other missing values, as interpolated values would be more informative for the models than values from other imputation methods. This decision could be revisited after models are built, to confirm that interpolated values are indeed more informative than another imputation method. It did require, however, that I separate the data into testing and training sets before imputing, in order not to have information leak between training and testing data. Because the dataset is so small, it would actually be better to impute 5 separate folds of data and use those to cross-validate performance. I haven’t done so in the interest of avoiding the refactoring time.BCP Visually scanning the raw CSV suggested that each attribute’s values were integers, so missing values could be found by specifying data type on import and checking for type conversion failures. No nulls were found after conversion to np.int64, so there were no missing values in the BCP dataset.website-phishing On visual inspection, all values seemed to be integers in the range -1,0,1. Converting to np.int64 and searching for nulls should flush out any missing values, assuming missing values were not represented by an integer in the range -1,0,1. No missing values were found in this dataset under that assumption. Some column names in this dataset included leading and trailing whitespace, so a simple strip and replace was conducted to eliminate that whitespace.Task 2 - Model ImplementationUtility Functionsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import cross_validatefrom sklearn.model_selection import GridSearchCVdef Tree(X_train, y_train, depth_range=[], max_depth=None): # if given a range of depths, cross-validate which is best if depth_range != []: temp_model = DecisionTreeClassifier(criterion=&#39;entropy&#39;, random_state=RANDOM_STATE) parameters = [{&#39;max_depth&#39;: depth_range}] cv = GridSearchCV(temp_model, parameters) cv.fit(X_train, y_train) best_depth = cv.best_params_[&#39;max_depth&#39;] else: best_depth = max_depth model = DecisionTreeClassifier(criterion=&#39;entropy&#39;, max_depth=best_depth, random_state=RANDOM_STATE) model.fit(X_train, y_train) return model def print_results(name, models, X_train, X_test, y_train, y_test): m = models[name] train_score = m.score(X_train, y_train) * 100 test_score = m.score(X_test, y_test) * 100 print(name, &#39;(depth {:})\\ttrain: {:.2f}%\\ttest: {:.2f}%&#39;.format(m.tree_.max_depth, train_score, test_score)) models = {} Code and Resultsarrhythmia.csvmodels[&#39;Arr Stump&#39;] = Tree(h_train_x, h_train_y, max_depth=1)print_results(&#39;Arr Stump&#39;, models, h_train_x, h_test_x, h_train_y, h_test_y)models[&#39;Arr Unpruned&#39;] = Tree(h_train_x, h_train_y)print_results(&#39;Arr Unpruned&#39;, models, h_train_x, h_test_x, h_train_y, h_test_y)models[&#39;Arr Pruned&#39;] = Tree(h_train_x, h_train_y, depth_range=range(1, 20))print_results(&#39;Arr Pruned&#39;, models, h_train_x, h_test_x, h_train_y, h_test_y)Arr Stump (depth 1) train: 55.96% test: 60.44%Arr Unpruned (depth 10) train: 100.00% test: 72.53%/Users/chase/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5. warnings.warn((&quot;The least populated class in y has only %d&quot;Arr Pruned (depth 5) train: 81.72% test: 71.43%BCP.csvX = bcp.drop(&#39;Class&#39;, axis=1)y = bcp.loc[:, &#39;Class&#39;]bcp_X_train, bcp_X_test, bcp_y_train, bcp_y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)# Fit models and display accuracy scoresmodels[&#39;BCP Stump&#39;] = Tree(bcp_X_train, bcp_y_train, max_depth=1)print_results(&#39;BCP Stump&#39;, models, bcp_X_train, bcp_X_test, bcp_y_train, bcp_y_test)models[&#39;BCP Unpruned&#39;] = Tree(bcp_X_train, bcp_y_train)print_results(&#39;BCP Unpruned&#39;, models, bcp_X_train, bcp_X_test, bcp_y_train, bcp_y_test)models[&#39;BCP Pruned&#39;] = Tree(bcp_X_train, bcp_y_train, depth_range=range(1,10))print_results(&#39;BCP Pruned&#39;, models, bcp_X_train, bcp_X_test, bcp_y_train, bcp_y_test)BCP Stump (depth 1) train: 92.12% test: 94.89%BCP Unpruned (depth 7) train: 100.00% test: 96.35%BCP Pruned (depth 6) train: 99.27% test: 96.35%website-phishing.csvX = website.drop(&#39;Class&#39;, axis=1)y = website.loc[:, &#39;Class&#39;]web_X_train, web_X_test, web_y_train, web_y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)# Fit models and display accuracy scoresmodels[&#39;Web Stump&#39;] = Tree(web_X_train, web_y_train, max_depth=1)print_results(&#39;Web Stump&#39;, models, web_X_train, web_X_test, web_y_train, web_y_test)models[&#39;Web Unpruned&#39;] = Tree(web_X_train, web_y_train)print_results(&#39;Web Unpruned&#39;, models, web_X_train, web_X_test, web_y_train, web_y_test)models[&#39;Web Pruned&#39;] = Tree(web_X_train, web_y_train, depth_range=range(10,30))print_results(&#39;Web Pruned&#39;, models, web_X_train, web_X_test, web_y_train, web_y_test)Web Stump (depth 1) train: 88.67% test: 89.78%Web Unpruned (depth 24) train: 99.06% test: 96.43%Web Pruned (depth 19) train: 98.69% test: 96.65%DiscussionI chose to use pre-pruning for the pruned model, because the decision stumps were already quite accurate, and it was very simple to implement a hard limit on the tree depth. However, pruning in this way did not greatly improve test performance over the unpruned models.Task 3 - Hyperparameter TuningDiscussionI chose to use exhaustive grid search with cross-validation to select my pre-pruning hyperparameter. This method iterates through each combination of hyperparameters in the specified range of options. For each hyperparameter option, the training data is split into 5 different training/validation folds. The model is separately fit and tested on each training/validation set combination (fold), and the performance scores of each fold are averaged out to get a single performance score for the hyperparameter option being used. By repeating this process for each hyperparameter option, the “best” hyperparameter for the data can be selected with minimal bias. Grid search was selected over random search because only one hyperparameter needed tuning, so the two operations would likely be equivalent in runtime and result.The arrhythmia dataset was optimised at a very shallow depth for the number of features included. I attributed this to an overabundance of features in the dataset relative to the number of examples. This assumption is strengthened by the very low test performance of all attempted models. There is simply not enough data to discover many relationships between the hundreds of features and the class.The bcp dataset was optimised near its maximum possible depth: nearly all of the features were at least somewhat informative. However, the high test performance of the decision stump suggests that only one feature contributed the vast majority of information necessary to make an accurate prediction.The website dataset turned out to be similar to bcp, though with many more features and many more examples. The decision stump scored quite well, with the unpruned and pre-pruned models using a high number of features to gain some small amount of new information.Task 4 - Comparing ModelsCode and Resultsfrom mlxtend.evaluate import paired_ttest_5x2cvfor m1 in [&#39;Arr Stump&#39;, &#39;Arr Unpruned&#39;]: for m2 in [&#39;Arr Unpruned&#39;, &#39;Arr Pruned&#39;]: if m1 == m2: # do not compare the unpruned model to itself continue t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=h_train_x, y=h_train_y, random_seed=RANDOM_STATE) flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot; print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)print()for m1 in [&#39;BCP Stump&#39;, &#39;BCP Unpruned&#39;]: for m2 in [&#39;BCP Unpruned&#39;, &#39;BCP Pruned&#39;]: if m1 == m2: # do not compare the unpruned model to itself continue t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=bcp_X_train, y=bcp_y_train, random_seed=RANDOM_STATE) flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot; print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)print()for m1 in [&#39;Web Stump&#39;, &#39;Web Unpruned&#39;]: for m2 in [&#39;Web Unpruned&#39;, &#39;Web Pruned&#39;]: if m1 == m2: # do not compare the unpruned model to itself continue t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=web_X_train, y=web_y_train, random_seed=RANDOM_STATE) flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot; print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)Arr Stump vs Arr Unpruned p value: 0.199 Arr Stump vs Arr Pruned p value: 0.231 Arr Unpruned vs Arr Pruned p value: 0.707 BCP Stump vs BCP Unpruned p value: 0.012 &amp;lt;--Significant differenceBCP Stump vs BCP Pruned p value: 0.011 &amp;lt;--Significant differenceBCP Unpruned vs BCP Pruned p value: 0.384 Web Stump vs Web Unpruned p value: 0.000 &amp;lt;--Significant differenceWeb Stump vs Web Pruned p value: 0.000 &amp;lt;--Significant differenceWeb Unpruned vs Web Pruned p value: 0.597 DiscussionSignificance tests were conducted using a 5x2cv paired test. For 5 iterations, the data was split into two 50% halves, with one half used to fit the two specified models, and other half used to evaluate each of the models. The models were then fit and tested again, but with the two halves of data swapped. The difference in prediction accuracy between the two models for each of the two train/test phases was computed, and a mean and variance for the set of two differences is computed. When all 5 iterations finished, the very first accuracy difference computed was used, along with the variance of the distribution of mean differences in accuracy, to compute the t statistic. The t statistic given 5 degrees of freedom yielded the p value, illuminating the statistical significance of the differences in prediction accuracy between models. Any reported p values below the significance threshold of 0.05 indicate a statistically significant difference in prediction accuracy.The differences in prediction accuracy between all three arrhythmia models were not statistically significant, because all three models performed so poorly. Likewise, a statistically significant difference between any unpruned vs pruned models trained on any dataset was not found, likely because the pre-pruning method was not able to remove any nodes without negatively affecting validation scores.There was a statistically significant difference in prediction accuracy between the decision stump model and the other two models, for both the BCP and website datasets. The decision stump was able to capture the most important feature in each dataset, and perform quite well as a result, but continuing to build the tree with more features made a significant difference on prediction accuracy.Task 5 - Different Pruning StrategyUtility Functionsfrom sklearn.model_selection import RandomizedSearchCVdef CostPrunedTree(X_train, y_train): temp_model = DecisionTreeClassifier(criterion=&#39;entropy&#39;, random_state=RANDOM_STATE) path = temp_model.cost_complexity_pruning_path(X_train, y_train) alphas = path.ccp_alphas[:-1] parameters = [ {&#39;ccp_alpha&#39;: alphas} ] n_iter = 10 if len(alphas) &amp;lt; 100 else int(len(alphas)**0.5) clf = RandomizedSearchCV(temp_model, parameters, n_iter=n_iter, random_state=RANDOM_STATE) clf.fit(X_train, y_train) best_alpha = clf.best_params_[&#39;ccp_alpha&#39;] model = DecisionTreeClassifier(criterion=&#39;entropy&#39;, ccp_alpha=best_alpha, random_state=RANDOM_STATE) model.fit(X_train, y_train) return modelCode and Resultsmodels[&#39;Arr CCPruned&#39;] = CostPrunedTree(h_train_x, h_train_y)print_results(&#39;Arr CCPruned&#39;, models, h_train_x, h_test_x, h_train_y, h_test_y)models[&#39;BCP CCPruned&#39;] = CostPrunedTree(bcp_X_train, bcp_y_train)print_results(&#39;BCP CCPruned&#39;, models, bcp_X_train, bcp_X_test, bcp_y_train, bcp_y_test)models[&#39;Web CCPruned&#39;] = CostPrunedTree(web_X_train, web_y_train)print_results(&#39;Web CCPruned&#39;, models, web_X_train, web_X_test, web_y_train, web_y_test)/Users/chase/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5. warnings.warn((&quot;The least populated class in y has only %d&quot;Arr CCPruned (depth 8) train: 83.10% test: 72.53%BCP CCPruned (depth 7) train: 99.45% test: 97.08%Web CCPruned (depth 23) train: 98.88% test: 96.56%# Arrhythmia pre-pruning vs cost-complexity pruningm1, m2 = &#39;Arr Pruned&#39;, &#39;Arr CCPruned&#39;t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=h_train_x, y=h_train_y, random_seed=RANDOM_STATE)flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot;print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)# BCP pre-pruning vs cost-complexity pruningm1, m2 = &#39;BCP Pruned&#39;, &#39;BCP CCPruned&#39;t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=bcp_X_train, y=bcp_y_train, random_seed=RANDOM_STATE)flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot;print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)# website-phishing pre-pruning vs cost-complexity pruningm1, m2 = &#39;Web Pruned&#39;, &#39;Web CCPruned&#39;t, p = paired_ttest_5x2cv(estimator1=models[m1], estimator2=models[m2], X=web_X_train, y=web_y_train, random_seed=RANDOM_STATE)flag = &quot;\\t&amp;lt;--Significant difference&quot; if p &amp;lt; 0.05 else &quot;&quot;print(m1, &#39;vs&#39;, m2, &#39;\\tp value: %.3f&#39; % p, flag)Arr Pruned vs Arr CCPruned p value: 0.863 BCP Pruned vs BCP CCPruned p value: 0.384 Web Pruned vs Web CCPruned p value: 0.573 DiscussionThe new pruning strategy executed was cost complexity pruning, with the complexity parameter (alpha) determined by random search cross-validation over at least ten possible values. Alpha acts like a misclassification threshold: nodes whose decisions cause misclassification at a rate higher than alpha are pruned, in order of severity of misclassification. This is done by calculating the cost complexity of each node, pruning the node with maximum cost complexity, and repeating the process until every node’s cost complexity is greater than alpha. Random cross-validation is used to select alpha because exhaustive search would take too long, and a random sample of all possible alphas of the right size should find an alpha that is good enough.Cost-complexity pruning did not yield significantly better results than simply restricting the depth of the decision tree. This is confirmed with paired t-test cv accuracy comparisons and their resulting high p-values. The insignificant improvement over pre-pruning likely comes from the datasets themselves: they are either too complex for a decision tree to be very accurate (arrhythmia), or they are so easily predictable by a decision tree that it does not matter much that the tree is pruned any certain way, or pruned at all (BCP and website)." }, { "title": "Welcome!", "url": "/posts/welcome/", "categories": "General", "tags": "personal", "date": "2022-04-21 00:00:00 +1200", "snippet": "Hello world. I probably won’t post much here, but please check out my Resume, CV, and About tabs for more info. My social and contact links can also be found at the bottom of the sidebar.Here’s a cute dog I found on the internet.Cheers :)" } ]
